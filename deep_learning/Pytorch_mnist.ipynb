{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pytorch_mnist.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/siddharth1608/datascience/blob/master/deep_learning/Pytorch_mnist.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "9epgFhgZOfvT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Handwritten Digit Classifier using CNN in Pytorch ##"
      ]
    },
    {
      "metadata": {
        "id": "PiL7qIDsX0Kl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In this notebook, I walk you through my understanding about Convolutional Neural Networks(CNNs) and how it is implemented in Pytorch\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**CNN are specialized for image processing. Why?**\n",
        "\n",
        "The architecture of a simple feed forward neural network consists of densely connected layers. By dense, I mean every node in the previous layer is connected to every node in the current layer. This complex nature allows to learn intricate functions from data. \n",
        "\n",
        "However, this architecture is not favourable for images. Imagine a 28 x 28 pixels image, fed into a neural network and passed through the layers. To learn the functions from every pixel, we would need extremely large number of nodes and it sounds like a computational nightmare to learn the network parameters. Enter, **CNN**.\n",
        "\n",
        "CNNs main idea is **convolution** operation.  Convolution is nothing but a tiny matrix(also called filter) that slides over the whole image and extracts features from a group of neighbouring pixels. For e.g. a network can learn weights for a filter to detect a circle in the image that would increase probability of 6, 8 or 9 in the image.\n",
        "\n",
        "A CNN network may contain multiple filters spread across multiple layers\n",
        "\n",
        "After, convolution, **pooling** is applied which is obtaining a summary value for a group of values. Popular pooling used is MaxPooling, which simply passes through the maximum value in a grid.\n",
        "\n",
        "With the help of convolution and pooling, CNNs summarize information instead of looking at every pixel."
      ]
    },
    {
      "metadata": {
        "id": "FpSnFch2dpAE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now, lets see the implementation of CNN in Pytorch"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "outputId": "b3c3dba7-d8f8-4bc1-eb41-740ffd7f74f8",
        "id": "7yn8YWlrCQBg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import PIL.Image as pil\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "print(torch.__version__)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.0.1.post2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "fsGGRLvjOjEt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "num_epochs = 5\n",
        "num_classes = 10\n",
        "batch_size = 100\n",
        "learning_rate = 0.001"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FqLE41u8Q4XW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "train_dataset = pd.read_csv('/content/sample_data/mnist_train_small.csv', header=None)\n",
        "test_dataset = pd.read_csv('/content/sample_data/mnist_test.csv', header=None)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ht4pebaYRtNu",
        "colab_type": "code",
        "outputId": "a2f68b40-6f4d-4daa-ed17-67d637ef76c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "print('Train shape {}, Test shape {}'.format(train_dataset.shape, test_dataset.shape))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train shape (20000, 785), Test shape (10000, 785)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "loBwpHVrTrGf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "View sample images"
      ]
    },
    {
      "metadata": {
        "id": "sGSi3ycbT-UA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "outputId": "a9690ede-8da7-4917-e2a3-e46804cee1a7"
      },
      "cell_type": "code",
      "source": [
        "first_image = train_dataset.iloc[0][1:].values.reshape(28,28).astype('uint8')\n",
        "\n",
        "### Using PIL\n",
        "#img = pil.fromarray(np.uint8(first_image * 255) , 'L')\n",
        "#img\n",
        "\n",
        "### Using matplot\n",
        "plt.imshow(first_image)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f38a86b2d68>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUsAAAFKCAYAAACU6307AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEheDswAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "j4Y9PaEIkrbP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The dataset has first column as the label. and image data as flattened, 784 cells - we need to make it in grid shape of size 28 x 28"
      ]
    },
    {
      "metadata": {
        "id": "enn6jEeD1tGi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In order to load our custom dataset into Pytorch and use DataLoader we have to inherit torch.DataSet class"
      ]
    },
    {
      "metadata": {
        "id": "TPGn6rUEmoXS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class CustomMNISTDatasetFromCSV(Dataset):\n",
        "  def __init__(self, file_path, image_height, image_width, transforms=None):\n",
        "    self.data = pd.read_csv(file_path, header=None)\n",
        "    self.labels = self.data.iloc[:,0]\n",
        "    self.image_height = image_height\n",
        "    self.image_width = image_width\n",
        "    self.transforms = transforms\n",
        "    \n",
        "  def __getitem__(self, index):\n",
        "    image_label = self.labels[index]\n",
        "    image_as_np_arr = np.asarray(self.data.iloc[index][1:]).reshape(self.image_height, self.image_width).astype('uint8')\n",
        "    # uint8 - unsigned int, 0-255\n",
        "    \n",
        "    if self.transforms is not None:\n",
        "        img_as_tensor = self.transforms(image_as_np_arr)\n",
        "        \n",
        "    return (img_as_tensor, image_label)\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.data.index)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IPSfbwskvfPP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Define transforms\n",
        "transformations = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.1307,), (0.3081,))])\n",
        "\n",
        "# Define custom dataset\n",
        "train_mnist_from_csv = CustomMNISTDatasetFromCSV('/content/sample_data/mnist_train_small.csv',28, 28,\n",
        "                         transformations)\n",
        "\n",
        "test_mnist_from_csv = CustomMNISTDatasetFromCSV('/content/sample_data/mnist_test.csv',28, 28,\n",
        "                         transformations)\n",
        "\n",
        "train_loader = DataLoader(dataset=train_mnist_from_csv, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "test_loader = DataLoader(dataset=test_mnist_from_csv, batch_size=batch_size, shuffle=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "X-9ByYoVK5Jg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now, we will create the Layers for our Convolutional Neural network"
      ]
    },
    {
      "metadata": {
        "id": "KmfT-_FBLCV6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class CNN(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(CNN,self).__init__();\n",
        "    \n",
        "    # Our first layer would consists of \n",
        "    # 1. Convolution Layer: 32 filters/channels of size 5x5 and stride of 1(slide unit)\n",
        "    # 2. Relu Layer\n",
        "    # 3. Pooling Layer\n",
        "    \n",
        "    self.layer1 = nn.Sequential(\n",
        "                      nn.Conv2d(1, 32, kernel_size=5, stride = 1, padding =2),\n",
        "                      nn.ReLU(),\n",
        "                      nn.MaxPool2d(kernel_size=2, stride = 2)\n",
        "                  )\n",
        "    \n",
        "    # Our second layer would consist of similar structure except that input will be 32 channels and output will be \n",
        "    # 64 channels\n",
        "    self.layer2 = nn.Sequential(\n",
        "                      nn.Conv2d(32, 64, kernel_size = 5, stride = 1, padding = 2),\n",
        "                      nn.ReLU(),\n",
        "                      nn.MaxPool2d(kernel_size =2, stride =2)\n",
        "                  )\n",
        "    \n",
        "    self.dropout = nn.Dropout()\n",
        "    \n",
        "    # Fully connected Layer network to flatten the channels - '7 * 7'(image size) * 64(no of channels)\n",
        "    self.fc1 = nn.Linear(7*7*64, 1000)\n",
        "    \n",
        "    # Last layer to assign values to labels, 0-9\n",
        "    self.fc2 = nn.Linear(1000, 10)\n",
        "    \n",
        "  def forward(self, x):\n",
        "    out = self.layer1(x)\n",
        "    out = self.layer2(out)\n",
        "\n",
        "    # Below we Reshape the layer2 output with Number of rows and unknown columns. This will flatten the 7 * 7 images of every 64 channels\n",
        "    out = out.reshape(out.shape[0],-1) \n",
        "\n",
        "    out = self.fc1(out)\n",
        "    out = self.fc2(out)\n",
        "\n",
        "    return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "le3AINCQbxs1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**How is the padding decided ?**\n",
        "\n",
        "${W_{out} = {\\frac{W_{in}-F + 2P}{S}} + 1  }$\n",
        "\n",
        "${W_{out} } $ is the dimension of output after convolution\n",
        "\n",
        "${ W_{in} } $ is the dimension of the input. \n",
        "\n",
        "${F}$ : Filter size\n",
        "\n",
        "${S }$: Stride\n",
        "\n",
        "${P}$: Padding\n",
        "\n",
        "We want to apply a filter 5 x5 and with a stride of 1. Also we want the input and output of same dimensions so we plug in these values in the above formula which suggests us to use Padding of 2.\n",
        "\n",
        "The same formula can be used to find padding for Pooling operation. We want to down sample the images from 28 \\* 28 to 14 \\* 14, which can be achieved by using a padding of 0"
      ]
    },
    {
      "metadata": {
        "id": "_SKhlsdhOI3z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Model Training**"
      ]
    },
    {
      "metadata": {
        "id": "FqEHkydsN0IV",
        "colab_type": "code",
        "outputId": "cf085625-dc22-4bd2-a6e2-d99a98c1a15f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "cell_type": "code",
      "source": [
        "model = CNN()\n",
        "\n",
        "# Loss and Optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "\n",
        "\n",
        "# Train the model\n",
        "total_step = len(train_loader)\n",
        "loss_list = []\n",
        "acc_list = []\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        # Run the forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss_list.append(loss.item())\n",
        "\n",
        "        # Backprop and perform Adam optimisation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Track the accuracy\n",
        "        total = labels.size(0)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        correct = (predicted == labels).sum().item()\n",
        "        acc_list.append(correct / total)\n",
        "\n",
        "        if (i + 1) % 100 == 0:\n",
        "            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Accuracy: {:.2f}%'\n",
        "                  .format(epoch + 1, num_epochs, i + 1, total_step, loss.item(),\n",
        "                          (correct / total) * 100))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch [1/5], Step [100/200], Loss: 0.1386, Accuracy: 93.00%\n",
            "Epoch [1/5], Step [200/200], Loss: 0.1327, Accuracy: 97.00%\n",
            "Epoch [2/5], Step [100/200], Loss: 0.0497, Accuracy: 99.00%\n",
            "Epoch [2/5], Step [200/200], Loss: 0.0406, Accuracy: 98.00%\n",
            "Epoch [3/5], Step [100/200], Loss: 0.0782, Accuracy: 99.00%\n",
            "Epoch [3/5], Step [200/200], Loss: 0.0466, Accuracy: 98.00%\n",
            "Epoch [4/5], Step [100/200], Loss: 0.0116, Accuracy: 100.00%\n",
            "Epoch [4/5], Step [200/200], Loss: 0.0455, Accuracy: 99.00%\n",
            "Epoch [5/5], Step [100/200], Loss: 0.0124, Accuracy: 100.00%\n",
            "Epoch [5/5], Step [200/200], Loss: 0.1364, Accuracy: 96.00%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "bLmam62t1irs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Test the model**"
      ]
    },
    {
      "metadata": {
        "id": "Jp7XrH_X1k8T",
        "colab_type": "code",
        "outputId": "833486a4-5210-465c-e103-b7f9108d7a54",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in test_loader:\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print('Test Accuracy of the model on the 10000 test images: {} %'.format((correct / total) * 100))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Accuracy of the model on the 10000 test images: 97.92 %\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}