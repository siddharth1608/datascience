{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Cloud code ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_word_cloud_from_freq(tagdict, suptitle, title = \"\", image_file = \"\", save_to_file=False, file_name = \"\", EXTRACT = ''):\n",
    "#     print(tagdict)\n",
    "    if tagdict:\n",
    "        \n",
    "        VERSION = 2\n",
    "#         df = dict(tagdict)\n",
    "        stopwords = set(STOPWORDS)\n",
    "        stopwords.add('traction')\n",
    "        stopwords.add('end')\n",
    "        stopwords.update()\n",
    "\n",
    "        if image_file:\n",
    "            wordcloud = WordCloud(max_font_size=40, min_font_size=8, stopwords=stopwords,margin=5,\n",
    "                              mask=traction_coloring,\n",
    "                              background_color=\"white\",\n",
    "                             ).generate_from_frequencies(tagdict)\n",
    "\n",
    "            # create coloring from image\n",
    "            image_colors = ImageColorGenerator(traction_coloring)\n",
    "            \n",
    "            plt.figure(figsize=(10,5))\n",
    "            plt.imshow(wordcloud.recolor(color_func=image_colors), interpolation=\"bilinear\")\n",
    "            plt.axis(\"off\")        \n",
    "            plt.suptitle(suptitle, fontsize=20)\n",
    "            plt.title(title, fontsize=15)            \n",
    "            \n",
    "        else:\n",
    "            wordcloud = WordCloud(max_font_size=40, min_font_size=8, stopwords=stopwords,margin=5,\n",
    "                              background_color=\"white\",\n",
    "                             ).generate_from_frequencies(tagdict)\n",
    "\n",
    "           \n",
    "            plt.figure(figsize=(10,5))\n",
    "            plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "            plt.axis(\"off\")        \n",
    "            plt.suptitle(suptitle, fontsize=20)\n",
    "            plt.title(title, fontsize=15)            \n",
    "        \n",
    "        if save_to_file:\n",
    "            fileName = file_name if file_name else (suptitle.lower()+'_'+title.lower()).replace(' ','_').replace('/','_').replace(':','_')\n",
    "#             wordcloud.to_file('../plots/qe2018/'+ fileName + '.png')  \n",
    "            plt.tight_layout()\n",
    "            plt.subplots_adjust(top=0.88)\n",
    "            filePath = path.join('../plots',EXTRACT, fileName+'.png')\n",
    "            plt.savefig(filePath)\n",
    "#             plt.savefig('../plots/'+ fileName +'.png')\n",
    "            \n",
    "    else:\n",
    "        return ''    \n",
    "    \n",
    "    \n",
    "def generate_word_cloud_from_text(text, suptitle, title = \"\", image_file = \"\", obvious_common_words = \"\", save_to_file=False, file_name = \"\", EXTRACT = ''):\n",
    "#     print(tagdict)\n",
    "    if text:\n",
    "        \n",
    "        VERSION = 2\n",
    "#         df = dict(tagdict)\n",
    "        stopwords = set(STOPWORDS)\n",
    "        stopwords.add('traction')\n",
    "        stopwords.add('end')\n",
    "        \n",
    "        for word in obvious_common_words:\n",
    "            stopwords.add(word)\n",
    "            \n",
    "        \n",
    "\n",
    "        if image_file:\n",
    "            wordcloud = WordCloud(max_font_size=40, min_font_size=8, stopwords=stopwords,margin=5,\n",
    "        #                       width=800, height=800,\n",
    "                              mask=traction_coloring,\n",
    "                              background_color=\"white\",\n",
    "#                               contour_color=\"steelblue\"\n",
    "                             ).generate(text)\n",
    "\n",
    "            # create coloring from image\n",
    "            image_colors = ImageColorGenerator(traction_coloring)\n",
    "            \n",
    "            plt.figure(figsize=(10,5))\n",
    "            plt.imshow(wordcloud.recolor(color_func=image_colors), interpolation=\"bilinear\")\n",
    "            plt.axis(\"off\")        \n",
    "            plt.suptitle(suptitle, fontsize=20)\n",
    "            plt.title(title, fontsize=15)\n",
    "#             plt.show()\n",
    "            \n",
    "        else:\n",
    "            wordcloud = WordCloud(max_font_size=40, min_font_size=8, stopwords=stopwords,margin=5,\n",
    "        #                       width=800, height=800,\n",
    "                              background_color=\"white\",\n",
    "                             ).generate(text)\n",
    "\n",
    "           \n",
    "            plt.figure(figsize=(10,5))\n",
    "            plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "            plt.axis(\"off\")        \n",
    "            plt.suptitle(suptitle, fontsize=20)\n",
    "            plt.title(title, fontsize=15)\n",
    "#             plt.show()\n",
    "        \n",
    "        if save_to_file:\n",
    "            fileName = file_name if file_name else (suptitle.lower()+'_'+title.lower()).replace(' ','_').replace('/','_').replace(':','_')\n",
    "#             wordcloud.to_file('../plots/qe2018/'+ fileName + '.png') \n",
    "            plt.tight_layout()\n",
    "            plt.subplots_adjust(top=0.88)\n",
    "            filePath = path.join('../plots',EXTRACT, fileName+'.png')\n",
    "            plt.savefig(filePath)\n",
    "#             plt.savefig('../plots/'+ fileName +'.png')\n",
    "        \n",
    "    else:\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findtags(tag_prefix, tagged_text, count_of_words = 10):\n",
    "\tcfd = nltk.ConditionalFreqDist((tag, word) for (word, tag) in tagged_text\n",
    "\t\t\t\t\t\t\t\t   if tag.startswith(tag_prefix))\n",
    "\treturn dict((tag, cfd[tag].most_common(count_of_words)) for tag in cfd.conditions())    \n",
    "    \n",
    "def derive_frequent_nouns_for_field(df, field, count_of_words = 10, stop_words = [], generate_word_cloud = False, word_cloud_params = dict(), min_word_length = 2):\n",
    "\n",
    "    concatenated_text = df[field].str.cat(sep='. ')\n",
    "\n",
    "    text = word_tokenize(concatenated_text.lower())\n",
    "    tagged_text = nltk.pos_tag(text)\n",
    "\n",
    "    tagdictNN = findtags('NN', tagged_text, count_of_words)        \n",
    "\n",
    "\n",
    "    nouns = tagdictNN.get('NNS') + tagdictNN.get('NN') + tagdictNN.get('NNP',[])\n",
    "\n",
    "    nounsDict = dict(nouns)      \n",
    "\n",
    "    # Add Stopwords\n",
    "    for stopword in stop_words:\n",
    "        if stopword in nounsDict:\n",
    "            nounsDict.pop(stopword)\n",
    "\n",
    "\n",
    "    # Sort the words by their frequency\n",
    "    nounsDict = dict(sorted(nounsDict.items(), key=lambda kv: kv[1], reverse=True))\n",
    "\n",
    "    if generate_word_cloud:\n",
    "        if word_cloud_params.get('suptitle') == \"\":\n",
    "            print('Need Title for the word cloud')\n",
    "        else:\n",
    "            generate_word_cloud_from_freq(nounsDict, word_cloud_params.get('suptitle'), word_cloud_params.get('title'), word_cloud_params.get('save_to_file'))\n",
    "    else:\n",
    "        return nounsDict\n",
    "\n",
    "def derive_nouns_for_list(values, count_of_words = 10, stop_words = [], generate_word_cloud = False, word_cloud_params = dict(), min_word_length = 2):\n",
    "        \n",
    "    text = word_tokenize(values.lower())\n",
    "    tagged_text = nltk.pos_tag(text)\n",
    "        \n",
    "    tagdictNN = findtags('NN', tagged_text, count_of_words)        \n",
    "\n",
    "\n",
    "    nouns = tagdictNN.get('NNS',[]) + tagdictNN.get('NN',[]) + tagdictNN.get('NNP',[])\n",
    "\n",
    "    nounsDict = dict(nouns)      \n",
    "\n",
    "    # Add Stopwords\n",
    "    for stopword in stop_words:\n",
    "        if stopword in nounsDict:\n",
    "            nounsDict.pop(stopword)\n",
    "\n",
    "\n",
    "    # Sort the words by their frequency\n",
    "    nounsDict = dict(sorted(nounsDict.items(), key=lambda kv: kv[1], reverse=True))\n",
    "\n",
    "    if generate_word_cloud:\n",
    "        if word_cloud_params.get('suptitle') == \"\":\n",
    "            print('Need Title for the word cloud')\n",
    "        else:\n",
    "            generate_word_cloud_from_freq(nounsDict, word_cloud_params.get('suptitle'), word_cloud_params.get('title'), word_cloud_params.get('save_to_file'))\n",
    "    else:\n",
    "        return nounsDict\n",
    "    \n",
    "def derive_frequent_adjectives_for_field(df, field, count_of_words = 10, stop_words = [], generate_word_cloud = False, word_cloud_params = dict()):\n",
    "\n",
    "    concatenated_text = df[field].str.cat(sep='. ')\n",
    "\n",
    "    text = word_tokenize(concatenated_text.lower())\n",
    "    tagged_text = nltk.pos_tag(text)\n",
    "\n",
    "    tagdictNN = findtags('JJ', tagged_text, count_of_words)        \n",
    "\n",
    "\n",
    "    adjs = tagdictNN.get('JJ')\n",
    "\n",
    "    adjsDict = dict(adjs)      \n",
    "\n",
    "    # Add Stopwords\n",
    "    for stopword in stop_words:\n",
    "        if stopword in adjsDict:\n",
    "            adjsDict.pop(stopword)\n",
    "\n",
    "\n",
    "    # Sort the words by their frequency\n",
    "    adjsDict = dict(sorted(adjsDict.items(), key=lambda kv: kv[1], reverse=True))\n",
    "\n",
    "    if generate_word_cloud:\n",
    "        if word_cloud_params.get('suptitle') == \"\":\n",
    "            print('Need Title for the word cloud')\n",
    "        else:\n",
    "            generate_word_cloud_from_freq(adjsDict, word_cloud_params.get('suptitle'), word_cloud_params.get('title'), word_cloud_params.get('save_to_file'))\n",
    "    else:\n",
    "        return adjsDict "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get Frequent Nouns on a text field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_text = df[text_field].str.cat(sep='. ')\n",
    "\n",
    "text = word_tokenize(concatenated_text.lower())\n",
    "tagged_text = nltk.pos_tag(text)\n",
    "\n",
    "tagdictNN = findtags('NN', tagged_text)        \n",
    "\n",
    "\n",
    "nouns = tagdictNN.get('NNS') + tagdictNN.get('NN') + tagdictNN.get('NNP',[])\n",
    "\n",
    "nounsDict = dict(nouns)      \n",
    "\n",
    "# Add Stopwords\n",
    "if 'nuts' in nounsDict:\n",
    "    nounsDict.pop('nuts')\n",
    "\n",
    "if 'drives' in nounsDict:\n",
    "    nounsDict.pop('drives')\n",
    "\n",
    "\n",
    "# Sort the words by their frequency\n",
    "nounsDict = dict(sorted(nounsDict.items(), key=lambda kv: kv[1], reverse=True))\n",
    "\n",
    "generate_word_cloud_from_freq(nounsDict, text_field, 'Frequent Nouns', save_to_file=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_stem(text,min_word_length = 3):\n",
    "    text = text.encode('latin').decode('utf-8','ignore')\n",
    "    text = text.replace('/', ' ')\n",
    "    text = text.replace('-',' ')\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent, language='english', preserve_line=False) if len(word) >= min_word_length ]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    stems = [stemmer.stem(t) for t in filtered_tokens]\n",
    "    return stems\n",
    "\n",
    "def tokenize_only(text, min_word_length = 3):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent) if len(word) >= min_word_length]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Timeline Plotting ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_timeline(info):   \n",
    "   \n",
    "    \n",
    "    \n",
    "    dates = df['date']\n",
    "    names = df['name']\n",
    "    \n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    import matplotlib.dates as mdates\n",
    "    from datetime import datetime\n",
    "\n",
    "    \n",
    "    \n",
    "    # dates = [datetime.strptime(ii, \"%Y-%m-%dT%H:%M:%SZ\") for ii in dates]\n",
    "\n",
    "    levels = np.array([-5, 5, -3, 3, -1, 1, -6, 6, -7,7, -8, 8, -9, 9, -10, 10, -11, 11, -12, 12])\n",
    "    fig, ax = plt.subplots(figsize=(len(dates)*1.5, 20))\n",
    "\n",
    "    # Create the base line\n",
    "    start = min(dates)\n",
    "    stop = max(dates)\n",
    "    \n",
    "    ax.plot((start, stop), (0, 0), 'k', alpha=.5)\n",
    "\n",
    "    # Iterate through releases annotating each one\n",
    "    for ii, (iname, idate) in tqdm.tqdm(enumerate(zip(names, dates))):\n",
    "        level = levels[ii % 20]\n",
    "        vert = 'top' if level < 0 else 'bottom'\n",
    "\n",
    "        ax.scatter(idate, 0, s=100, facecolor='w', edgecolor='k', zorder=9999)\n",
    "        # Plot a line up to the text\n",
    "        ax.plot((idate, idate), (0, level), c='r', alpha=.7)\n",
    "        \n",
    "        if iname.find(text_you_want_to_format)!=-1:\n",
    "            color = 'red'\n",
    "        else:\n",
    "            color = 'black'\n",
    "            \n",
    "        # Give the text a faint background and align it properly\n",
    "        ax.text(idate, level, iname,\n",
    "                horizontalalignment='right', verticalalignment=vert, fontsize=14,\n",
    "                backgroundcolor=(1., 1., 1., .3), color=color)\n",
    "    ax.set(title=title)\n",
    "    \n",
    "    # Set the xticks formatting\n",
    "    # format xaxis with 3 month intervals\n",
    "    ax.get_xaxis().set_major_locator(mdates.MonthLocator(interval=3))\n",
    "    ax.get_xaxis().set_major_formatter(mdates.DateFormatter(\"%b %Y\"))\n",
    "    fig.autofmt_xdate()\n",
    "    \n",
    "    # Remove components for a cleaner look\n",
    "    plt.setp((ax.get_yticklabels() + ax.get_yticklines() +\n",
    "              list(ax.spines.values())), visible=False)\n",
    "    plt.savefig(filepath, bbox_inches='tight')\n",
    "#     plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Pre-process'''\n",
    "data_procsd_df = preprocess_detailed(data_for_model, \\\n",
    "                                  numeric_cols=numeric_cols, \n",
    "                                  label_encode_cols = label_encode_cols, \\\n",
    "                                  dummies_cols=dummies_cols, \\\n",
    "                                  dummies_cols_prefix=dummies_cols_prefix,\n",
    "                                  normalizeFlag = False,\n",
    "                                  standardizeFlag=False\n",
    "                                 )\n",
    "\n",
    "'''Getting names of new dummies column'''\n",
    "\n",
    "dummies_cols_prefixed = getDummyColumnNamesWithPrefix(df=data_procsd_df,\n",
    "                                                      dummies_cols=dummies_cols, \n",
    "                                                      dummies_cols_prefix=dummies_cols_prefix)\n",
    "\n",
    "\n",
    "data_procsd_df = data_procsd_df.rename(index=str, columns={\"ID\":\"dataId\"})\n",
    "id_col = 'dataId'\n",
    "'''Extract normal columns, dummy columns as train data'''\n",
    "data_procsd_df= data_procsd_df[[id_col]+features_cols+dummies_cols_prefixed]\n",
    "\n",
    "\n",
    "'''Add the target column - '''\n",
    "data_procsd_df = pd.concat([data_procsd_df.reset_index(drop=True), data_for_model['Left'].reset_index(drop=True)],axis=1)\n",
    "print('After adding target column', data_procsd_df.shape)\n",
    "\n",
    "print(data_procsd_df.shape)\n",
    "\n",
    "'''Introduce interaction terms'''\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "poly = PolynomialFeatures(interaction_only=True,include_bias = False)\n",
    "# interation_terms_df = pd.DataFrame(poly.fit_transform(data_procsd_df[features_cols+dummies_cols_prefixed]))\n",
    "# interation_terms_df = interactionterms(data_procsd_df[features_cols+dummies_cols_prefixed], include_original_features = False)\n",
    "\n",
    "''' ========================Feature Selection============================== '''\n",
    "\n",
    "'''Feature Selection'''\n",
    "\n",
    "NUM_OF_FEATURES = 7\n",
    "NUM_OF_FEATURES = len(features_cols+dummies_cols_prefixed) if len(features_cols+dummies_cols_prefixed) < NUM_OF_FEATURES else NUM_OF_FEATURES\n",
    "\n",
    "target_names = ['0','1']\n",
    "\n",
    "metrics = []\n",
    "\n",
    "best_estimators = []\n",
    "\n",
    "max_auc = 0\n",
    "max_recall = 0\n",
    "max_f1 = 0\n",
    "max_mathews = -1\n",
    "\n",
    "# X, y = interation_terms_df, data_procsd_df[target_col]\n",
    "X, y = data_procsd_df[features_cols+dummies_cols_prefixed], data_procsd_df[target_col]\n",
    "\n",
    "from imblearn.combine import SMOTEENN\n",
    "from imblearn.over_sampling import SMOTE, SMOTENC\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from collections import Counter\n",
    "rm = SMOTEENN(random_state=seed)\n",
    "\n",
    "# train_ids, test_ids = traintestidssplit(df=data_procsd_df,id_col=id_col,test_perc=0.33)\n",
    "# X_TRAIN = data_procsd_df[data_procsd_df[id_col].isin(train_ids)].drop(id_col,axis=1).drop(target_col, axis=1)\n",
    "# X_VALIDATION = data_procsd_df[data_procsd_df[id_col].isin(test_ids)].drop(id_col,axis=1).drop(target_col, axis=1)\n",
    "\n",
    "# y_TRAIN = data_procsd_df[data_procsd_df[id_col].isin(train_ids)][target_col] \n",
    "# y_VALIDATION = data_procsd_df[data_procsd_df[id_col].isin(test_ids)][target_col]\n",
    "\n",
    "X_TRAIN, X_VALIDATION, y_TRAIN, y_VALIDATION = train_test_split( X, y, test_size=0.33, random_state=seed)\n",
    "\n",
    "X, y = rm.fit_resample(X_TRAIN, y_TRAIN)\n",
    "\n",
    "print('Resampled dataset shape %s', y.shape)\n",
    "\n",
    "for feature in range(NUM_OF_FEATURES, NUM_OF_FEATURES+1):\n",
    "\n",
    "    \n",
    "#     X, y = interation_terms_df, data_procsd_df[target_col]\n",
    "#     X, y = interation_terms_df, data_procsd_df[target_col]   #With Manual Interaction terms\n",
    "\n",
    "    # '''Feature Selection'''\n",
    "#     selector = SelectKBest(mutual_info_classif, k=feature)    \n",
    "#     selector = selector.fit(X,y)\n",
    "    \n",
    "#     X_best = selector.transform(X)\n",
    "    \n",
    "#     pca = PCA(n_components=feature)\n",
    "#     X_best = pca.fit_transform(X)\n",
    "\n",
    "    \n",
    "    # '''Test-Train split using ids -  Only selected features'''\n",
    "#     data_procsd_df = data_procsd_df.rename(index=str, columns={\"ID\":\"dataId\"})\n",
    "#     id_col = 'dataId'\n",
    "\n",
    "#     X_best_df = pd.DataFrame(X_best)\n",
    "\n",
    "#     X_best_df = pd.concat([data_procsd_df[id_col].reset_index(),pd.DataFrame(X_best)], axis=1).drop(['index'],axis=1)\n",
    "# #     print(X_best_df.head(2))\n",
    "\n",
    "#     train_ids, test_ids = traintestidssplit(df=X_best_df,id_col=id_col,test_perc=0.33)\n",
    "#     X_train = X_best_df[X_best_df[id_col].isin(train_ids)].drop(id_col,axis=1)\n",
    "#     X_test = X_best_df[X_best_df[id_col].isin(test_ids)].drop(id_col,axis=1)\n",
    "\n",
    "#     y_train = data_procsd_df[data_procsd_df[id_col].isin(train_ids)][target_col] \n",
    "#     y_test = data_procsd_df[data_procsd_df[id_col].isin(test_ids)][target_col] \n",
    "\n",
    "    # Keep the below line only for imbalanced\n",
    "    X_train = X\n",
    "    y_train = y\n",
    "    \n",
    "    X_test = X_VALIDATION\n",
    "    y_test = y_VALIDATION\n",
    "    \n",
    "    print('Training size - {0} records\\n Testing size - {1} records'.format(X_train.shape[0], X_test.shape[0]))\n",
    "    print(y_test['Left'].value_counts())\n",
    "    \n",
    "    names = [\n",
    "#         \"Logistic Regression\",\n",
    "#              \"Nearest Neighbors\", \n",
    "#              \"SVM\", \n",
    "             \"Decision Tree\",\n",
    "             \"Random Forest\", \n",
    "#              \"AdaBoost\", \n",
    "#              \"Naive Bayes\", \n",
    "#              \"LDA\", \n",
    "#              \"QDA\",\n",
    "#         \"XGBoost\"\n",
    "            ]\n",
    "    \n",
    "    params = dict()\n",
    "#     params[\"Logistic Regression\"] = {\n",
    "#                                     'C':[1.0,2.5,3.5,5,10],\n",
    "#                                     'tol':[0.001,0.1,1],\n",
    "#                                    'class_weight':[{1:50,0:10},{1:100,0:50}],\n",
    "#                                    'solver' : ('newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga')\n",
    "#                                    }\n",
    "#     params[\"Nearest Neighbors\"] = {'n_neighbors':[1,2,3,4,5,6,7]}    \n",
    "#     params[\"SVM\"] = {'C':[1,10],'kernel':('linear', 'rbf'),'class_weight':[{1:100,0:10}]}\n",
    "    params[\"Decision Tree\"] = {'max_depth':[3, 5,10,20,30,40], \n",
    "                               'criterion':('entropy','gini'),\n",
    "                               'min_samples_split':[5, 10,20,30],'min_samples_leaf':[5,7],\n",
    "                               'class_weight':[{1:50,0:10}],\n",
    "                              'presort':(True, False)}\n",
    "    params[\"Random Forest\"] = {'max_depth':[3, 5,10, 20, 20, 40],\n",
    "                               'criterion':('entropy','gini'),'min_samples_split':[5, 10,20,30], 'min_samples_leaf':[5,10,15],\n",
    "                               'class_weight':[{1:50,0:10}],'n_estimators':[2,5,10],\n",
    "                              }\n",
    "#     params[\"AdaBoost\"] = {'learning_rate':[0.01,0.1,0.9,10]}\n",
    "#     params[\"Naive Bayes\"] = {}\n",
    "#     params[\"LDA\"] = {'solver':('svd','lsqr','eigen')}\n",
    "#     params[\"QDA\"] = {'reg_param':[0.1,0.9,10,100]}\n",
    "#     params['XGBoost'] = {'max_depth':[3, 5,10,20], \n",
    "# #                                ' n_estimators':[10,100],\n",
    "#                                'min_child_weight':[0.5,1],                               \n",
    "#                                 'scale_pos_weight':[1,10]}\n",
    "    \n",
    "\n",
    "    classifiers = [\n",
    "        \n",
    "#         LogisticRegression(random_state=seed),\n",
    "#         KNeighborsClassifier(3),\n",
    "#         SVC(random_state=seed, class_weight={1:100,0:10}),\n",
    "#         SVC(gamma=2, C=1, random_state=seed, class_weight={1:100,0:10}),\n",
    "        DecisionTreeClassifier(random_state=seed),\n",
    "        RandomForestClassifier(random_state=seed),\n",
    "#         AdaBoostClassifier(random_state=seed),\n",
    "#         GaussianNB(),\n",
    "#         LinearDiscriminantAnalysis(),\n",
    "#         QuadraticDiscriminantAnalysis(),\n",
    "#         XGBClassifier(random_state=seed),\n",
    "    ]\n",
    "\n",
    "    print('Features '+ str(feature))\n",
    "        \n",
    "     # iterate over classifiers\n",
    "    for name, clf in zip(names, classifiers):\n",
    "        \n",
    "        result_parameters = dict()\n",
    "\n",
    "         # Stratified Sampling\n",
    "#         skf = StratifiedKFold(n_splits=3)\n",
    "        \n",
    "    # ===== GridSearchCV  ======\n",
    "        \n",
    "        grid_search = GridSearchCV(clf, param_grid=params[name], \n",
    "#                                    cv=skf, \n",
    "                                   scoring='f1')\n",
    "        grid_search.fit(X_train, np.ravel(y_train))\n",
    "        \n",
    "        # make the predictions on train data\n",
    "#         y_pred = grid_search.predict(X_train.values)        \n",
    "        print(grid_search.best_params_)\n",
    "        \n",
    "        # make predictions on test data with best model\n",
    "        best_estimators.append(grid_search.best_estimator_)\n",
    "        y_pred = grid_search.best_estimator_.predict(X_test)\n",
    "        (tn, fp), (fn, tp) = confusion_matrix(y_test, y_pred)\n",
    "        accuracy = accuracy_score(np.ravel(y_test), np.ravel(y_pred))\n",
    "        recall = recall_score(np.ravel(y_test), np.ravel(y_pred))\n",
    "        f1 = f1_score(np.ravel(y_test), np.ravel(y_pred))\n",
    "        matthews_corr_coeff = matthews_corrcoef(np.ravel(y_test), np.ravel(y_pred))\n",
    "        \n",
    "        fpr, tpr, thresholds = roc_curve(np.ravel(y_test), np.ravel(y_pred),pos_label=1)\n",
    "        \n",
    "        import datetime        \n",
    "        result_parameters['date'] = datetime.datetime.now().strftime(\"%I:%M%p %B %d, %Y\")\n",
    "        result_parameters['num_of_features'] = feature\n",
    "        result_parameters['model'] = name\n",
    "        result_parameters['run_type'] = 'Grid-search test'\n",
    "        result_parameters['accuracy'] = accuracy\n",
    "        result_parameters['recall'] = recall\n",
    "        result_parameters['f1'] = f1\n",
    "        result_parameters['mathews_corr_coef'] = matthews_corr_coeff        \n",
    "        result_parameters['tp'] = tp\n",
    "        result_parameters['fp'] = fp\n",
    "        result_parameters['fn'] = fn\n",
    "        result_parameters['tn'] = tn\n",
    "        result_parameters['fpr'] = fpr\n",
    "        result_parameters['tpr'] = tpr\n",
    "        result_parameters['auc'] = auc(fpr, tpr)\n",
    "#         result_parameters['features'] =  X.iloc[:,selector.get_support(indices=True)].columns ###<- can't be used with imbalanced        \n",
    "        result_parameters['model_params'] = grid_search.best_params_\n",
    "        \n",
    "        metrics.append(result_parameters)\n",
    "#     # ===== GridSearchCV  ======\n",
    "        \n",
    "#         if result_parameters['auc'] > max_auc:\n",
    "#             persist_model_to_file(grid_search.best_estimator_,'pickles/data_attrition_predictor.pkl')\n",
    "#             print(result_parameters['model'],' persisted')\n",
    "#             max_auc = result_parameters['auc']\n",
    "            \n",
    "#         if result_parameters['recall'] > max_recall:\n",
    "#             persist_model_to_file(grid_search.best_estimator_,'pickles/data_attrition_predictor.pkl')\n",
    "#             print(result_parameters['model'],' persisted')\n",
    "#             max_recall = result_parameters['recall']\n",
    "            \n",
    "            \n",
    "        if result_parameters['f1'] > max_f1:\n",
    "            persist_model_to_file(grid_search.best_estimator_,'pickles/data_attrition_predictor.pkl')\n",
    "            print(result_parameters['model'],' persisted')\n",
    "            max_f1 = result_parameters['f1']\n",
    "        \n",
    "#         if result_parameters['mathews_corr_coef'] > max_mathews:\n",
    "#             persist_model_to_file(grid_search.best_estimator_,'pickles/data_attrition_predictor.pkl')\n",
    "#             print(result_parameters['model'],' persisted')\n",
    "#             max_mathews = result_parameters['mathews_corr_coef']\n",
    "        \n",
    "    print()\n",
    "    \n",
    "metrics_df = pd.DataFrame(metrics)\n",
    "\n",
    "plot_classification_result(metrics_df,names)\n",
    "\n",
    "# Update the metrics for the evaluation\n",
    "updateMetricsSheet(metrics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Decision Trees ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_names = X.iloc[:,[2, 3, 6, 7, 11, 22, 28]].columns\n",
    "# print(feature_names)\n",
    "clf = load_model_from_file('pickles/data_attrition_predictor.pkl')\n",
    "clf.feature_importances_\n",
    "\n",
    "\n",
    "\n",
    "'''This works only with Decision Trees'''\n",
    "from sklearn import tree\n",
    "import graphviz\n",
    "clf = load_model_from_file('pickles/data_attrition_predictor.pkl')\n",
    "dot_data = tree.export_graphviz(clf,out_file=None, \n",
    "                         feature_names=feature_names,  \n",
    "                         class_names=['Voluntary','No'],  \n",
    "                         filled=True, rounded=True,  \n",
    "                         special_characters=True)\n",
    "\n",
    "graph = graphviz.Source(dot_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''This works only with Random Forest'''\n",
    "from sklearn import tree\n",
    "import graphviz\n",
    "clf = load_model_from_file('pickles/data_attrition_predictor.pkl')\n",
    "graphs = []\n",
    "for index in range(0,len(clf.estimators_)):\n",
    "    dot_data = tree.export_graphviz(clf.estimators_[index],out_file=None, \n",
    "                             feature_names=feature_names,  \n",
    "                             class_names=['Voluntary','No'],  \n",
    "                             filled=True, rounded=True,  \n",
    "                             special_characters=True)\n",
    "\n",
    "    graph = graphviz.Source(dot_data)\n",
    "    graphs.append(graph)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
